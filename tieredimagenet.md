---
title: tieredImageNet
datatable: true
layout: leaderboard
---


## *tiered*ImageNet Leaderboard

Method   | Venue | Year | Backbone   | Learning | 1-shot      | 5-shot   | Code | Reported by 
------- | ------ | ---- | --------   | -----    | -----   | -----    | ---- | ----
[MetaOptNet](https://arxiv.org/pdf/1904.03758.pdf)     | CVPR   | 2019 | ResNet-12  | Inductive |  65.99 ± 0.72    | 81.56 ± 0.53     | [\[PyTorch\]](https://github.com/kjunelee/MetaOptNet) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[LEO](https://arxiv.org/pdf/1807.05960.pdf) | ICLR | 2019 | WRN-28-10 | Inductive | 66.33 ± 0.05 | 82.06 ± 0.08 | [\[TensorFlow\]](https://github.com/deepmind/leo) | [\[Source\]](https://arxiv.org/pdf/1807.05960.pdf)
[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | 4CONV | Inductive | 53.31 ± 0.89 | 72.69 ± 0.74 | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://arxiv.org/pdf/1703.05175.pdf)
[RelationNets](https://arxiv.org/pdf/1711.06025.pdf) | CVPR | 2018 | 4CONV | Inductive | 54.48 ± 0.93 | 71.32 ± 0.78  | [\[PyTorch\]](https://github.com/floodsung/LearningToCompare_FSL) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[TPN](https://arxiv.org/pdf/1805.10002.pdf) | ICLR | 2019 | 4CONV | Transductive | 59.91 ± 0.94 |  73.30 ± 0.75  | [\[TensorFlow\]](https://github.com/csyanbin/TPN) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[DeepEMD](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Inductive |  71.16 ± 0.87 | 86.03 ± 0.58  | [\[PyTorch\]](https://github.com/icoz69/DeepEMD) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | ResNet-12 | Inductive | 65.65 ± 0.92 | 83.40. ± 0.65 | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[MatchingNets](https://arxiv.org/pdf/1606.04080.pdf) | NeurIPS | 2016 | ResNet-12 | Inductive | 68.50 ± 0.92 | 80.60 ± 0.71  | [\[TensorFlow\]](https://github.com/AntreasAntoniou/MatchingNetworks) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[CTM](https://arxiv.org/pdf/1905.11116.pdf) | CVPR | 2019 | ResNet-18 | Inductive | 68.41 ± 0.39 | 84.28 ± 1.73 | [\[PyTorch\]](https://github.com/Clarifai/few-shot-ctm) | [\[Source\]](https://arxiv.org/pdf/1905.11116.pdf)
[wDAE-GNN](https://arxiv.org/pdf/1905.01102.pdf) | CVPR | 2019 | WRN-28-10 | Inductive | 68.18 ± 0.16 | 83.09 ± 0.12  | [\[PyTorch\]](https://github.com/gidariss/wDAE_GNN_FewShot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[PPA](https://arxiv.org/pdf/1706.03466.pdf) | CVPR | 2018 | WRN-28-10 | Inductive | 65.65 ± 0.92 | 83.40. ± 0.65  | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[CAN](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Inductive |  69.89 ± 0.51 | 84.23 ± 0.37 | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
[CAN+T](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Transductive |  73.21 ± 0.58 | 84.93 ± 0.38  | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | WRN-28-10 | Inductive |  70.41 ± 0.23 | 84.38 ± 0.16 | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | ResNet-12 | Inductive |  70.80 ± 0.23  | 84.79 ± 0.16 | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)